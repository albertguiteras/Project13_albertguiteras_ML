{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando dataset 1\n",
      "Procesando dataset 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [ 3  4 10 14] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando dataset 3\n",
      "Procesando dataset 4\n",
      "Procesando dataset 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [ 3  4 10 14] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:111: UserWarning: Features [ 3  4 10 14] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando dataset 6\n",
      "Dataset 1 - Train Accuracy: 90.9168%, Test Accuracy: 90.2865%\n",
      "Dataset 2 - Train Accuracy: 89.9545%, Test Accuracy: 89.4366%\n",
      "Dataset 3 - Train Accuracy: 90.9107%, Test Accuracy: 90.2987%\n",
      "Dataset 4 - Train Accuracy: 89.9393%, Test Accuracy: 89.4366%\n",
      "Dataset 5 - Train Accuracy: 90.8743%, Test Accuracy: 90.1287%\n",
      "Dataset 6 - Train Accuracy: 89.9514%, Test Accuracy: 89.4245%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import json\n",
    "\n",
    "BASE_PATH = \"../data/processed\"\n",
    "TRAIN_PATHS = [\n",
    "    \"X_train_con_outliers.xlsx\",\n",
    "    \"X_train_sin_outliers.xlsx\",\n",
    "    \"X_train_con_outliers_norm.xlsx\",\n",
    "    \"X_train_sin_outliers_norm.xlsx\",\n",
    "    \"X_train_con_outliers_scal.xlsx\",\n",
    "    \"X_train_sin_outliers_scal.xlsx\"\n",
    "]\n",
    "TRAIN_DATASETS = []\n",
    "for path in TRAIN_PATHS:\n",
    "    TRAIN_DATASETS.append(\n",
    "        pd.read_excel(os.path.join(BASE_PATH, path))\n",
    "    )\n",
    "\n",
    "TEST_PATHS = [\n",
    "    \"X_test_con_outliers.xlsx\",\n",
    "    \"X_test_sin_outliers.xlsx\",\n",
    "    \"X_test_con_outliers_norm.xlsx\",\n",
    "    \"X_test_sin_outliers_norm.xlsx\",\n",
    "    \"X_test_con_outliers_scal.xlsx\",\n",
    "    \"X_test_sin_outliers_scal.xlsx\"\n",
    "]\n",
    "TEST_DATASETS = []\n",
    "for path in TEST_PATHS:\n",
    "    TEST_DATASETS.append(\n",
    "        pd.read_excel(os.path.join(BASE_PATH, path))\n",
    "    )\n",
    "\n",
    "y_train = pd.read_excel(os.path.join(BASE_PATH, \"y_train.xlsx\"))\n",
    "y_test = pd.read_excel(os.path.join(BASE_PATH, \"y_test.xlsx\"))\n",
    "\n",
    "# Asegurarse de que y_train y y_test sean vectores 1D\n",
    "y_train = y_train.values.ravel()  # Convertir a 1D si es necesario\n",
    "y_test = y_test.values.ravel()    # Convertir a 1D si es necesario\n",
    "\n",
    "results = []\n",
    "\n",
    "# Definir el número de características que deseas seleccionar\n",
    "k = 5  # Puedes cambiar este valor según el número de características que desees seleccionar\n",
    "\n",
    "for index, dataset in enumerate(TRAIN_DATASETS):\n",
    "    print(f\"Procesando dataset {index + 1}\")\n",
    "\n",
    "    # Selección de características (usando SelectKBest)\n",
    "    selector = SelectKBest(f_classif, k=k)  # Selecciona las k mejores características usando f_classif\n",
    "    X_train_selected = selector.fit_transform(dataset, y_train)  # Ajuste y transformación para el conjunto de entrenamiento\n",
    "    X_test_selected = selector.transform(TEST_DATASETS[index])  # Transformación para el conjunto de prueba\n",
    "\n",
    "    # Guardar las características seleccionadas en un archivo JSON\n",
    "    selected_features = dataset.columns[selector.get_support()].tolist()  # Obtener los nombres de las características seleccionadas\n",
    "    selected_features_filename = f\"selected_features_{index + 1}_k_{k}.json\"  # Nombre del archivo JSON\n",
    "    with open(f\"../models/feature_selection{k}\", 'w') as json_file:\n",
    "        json.dump(selected_features, json_file)\n",
    "\n",
    "    # Entrenamiento del modelo con las características seleccionadas\n",
    "    model = LogisticRegression(random_state=42, max_iter=10000)  # Aumentar max_iter\n",
    "    model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_train = model.predict(X_train_selected)\n",
    "    y_pred_test = model.predict(X_test_selected)\n",
    "\n",
    "    # Almacenar los resultados de precisión\n",
    "    results.append(\n",
    "        {\n",
    "            \"train\": accuracy_score(y_train, y_pred_train),\n",
    "            \"test\": accuracy_score(y_test, y_pred_test)\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Imprimir los resultados\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Dataset {i+1} - Train Accuracy: {result['train'] * 100:.4f}%, Test Accuracy: {result['test'] * 100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3840 candidates, totalling 11520 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 90.9801\n",
      "Test Accuracy: 90.9323\n"
     ]
    }
   ],
   "source": [
    "### SPLIT 0.4\n",
    "\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np \n",
    "\n",
    "# Asumiendo que TRAIN_DATASETS y TEST_DATASETS son tus conjuntos de datos preprocesados y y_train, y_test son tus etiquetas\n",
    "\n",
    "# Ejemplo de mejor dataset (esto depende de tu código anterior)\n",
    "best_dataset_index = 2  # Esto lo debes definir según el conjunto de datos que hayas seleccionado como el mejor\n",
    "\n",
    "# Define tu modelo y el grid search (si estás haciendo GridSearchCV)\n",
    "model = LogisticRegression(random_state=42, solver='sag', tol=0.1)\n",
    "\n",
    "# Parámetros para GridSearchCV (esto es solo un ejemplo, ajusta según tus necesidades)\n",
    "param_grid = {\n",
    "    \"C\": np.logspace(-3, 3, 5),  # Reducir a 5 valores para C\n",
    "    \"penalty\": [\"l1\", \"l2\"],  # Probar solo l1 y l2\n",
    "    \"solver\": ['liblinear', 'saga'],  # Limitar a 'liblinear' y 'saga', que son más rápidos\n",
    "    \"fit_intercept\": [True, False],  # Mantener el ajuste de la intercepción\n",
    "    \"dual\": [False],  # 'dual=True' solo con 'liblinear', por lo que se mantiene en False\n",
    "    \"class_weight\": [None, \"balanced\"],  # Probar con 'None' y 'balanced'\n",
    "    \"multi_class\": ['auto', 'ovr'],  # Reducir a combinaciones más comunes\n",
    "    'tol': [1e-4, 1e-3],  # Reducir a solo dos valores para tolerancia\n",
    "    'max_iter': [100, 500],  # Reducir el número de iteraciones a 2 opciones\n",
    "    'warm_start': [True, False],  # Continuar desde la última solución\n",
    "    'l1_ratio': np.linspace(0, 1, 3)  # Reducir a 3 valores para l1_ratio\n",
    "}\n",
    "\n",
    "# GridSearchCV con menos combinaciones de parámetros\n",
    "grid = GridSearchCV(\n",
    "    model, param_grid=param_grid, cv=3,  # Usar 3 pliegues para validación cruzada\n",
    "    n_jobs=-1,  # Utilizar todos los núcleos del procesador\n",
    "    verbose=3,  # Mayor nivel de detalle en la salida de la búsqueda\n",
    "    scoring='accuracy',  # Optimizar para precisión\n",
    ")\n",
    "\n",
    "# Ajustamos el grid search al conjunto de datos seleccionado\n",
    "grid.fit(TRAIN_DATASETS[best_dataset_index], y_train)\n",
    "\n",
    "# Obtener el mejor modelo después de GridSearchCV\n",
    "final_model = grid.best_estimator_\n",
    "\n",
    "# Predicciones sobre el conjunto de entrenamiento\n",
    "y_pred_train = final_model.predict(TRAIN_DATASETS[best_dataset_index])\n",
    "\n",
    "# Predicciones sobre el conjunto de prueba\n",
    "y_pred_test = final_model.predict(TEST_DATASETS[best_dataset_index])\n",
    "\n",
    "# Calcular la exactitud\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Guardar los hiperparámetros y las métricas en un archivo JSON\n",
    "data_to_save = {\n",
    "    \"hyperparams\": grid.best_params_,  # Los mejores hiperparámetros encontrados\n",
    "    \"train_accuracy\": train_accuracy,\n",
    "    \"test_accuracy\": test_accuracy\n",
    "}\n",
    "\n",
    "# Guardamos los resultados en un archivo JSON\n",
    "with open(\"../data/hyperparams/hyperparams_2.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f, indent=4)\n",
    "\n",
    "# Imprimir las métricas de rendimiento\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPLIT 0.4\n",
    "\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np \n",
    "\n",
    "# Asumiendo que TRAIN_DATASETS y TEST_DATASETS son tus conjuntos de datos preprocesados y y_train, y_test son tus etiquetas\n",
    "\n",
    "# Ejemplo de mejor dataset (esto depende de tu código anterior)\n",
    "best_dataset_index = 2  # Esto lo debes definir según el conjunto de datos que hayas seleccionado como el mejor\n",
    "\n",
    "# Define tu modelo y el grid search (si estás haciendo GridSearchCV)\n",
    "model = LogisticRegression(random_state=42, solver='sag', tol=0.1)\n",
    "\n",
    "# Parámetros para GridSearchCV (esto es solo un ejemplo, ajusta según tus necesidades)\n",
    "param_grid = {\n",
    "    \"C\": np.logspace(-3, 3, 5),  # Reducir a 5 valores para C\n",
    "    \"penalty\": [\"l1\", \"l2\"],  # Probar solo l1 y l2\n",
    "    \"solver\": ['liblinear', 'saga'],  # Limitar a 'liblinear' y 'saga', que son más rápidos\n",
    "    \"fit_intercept\": [True, False],  # Mantener el ajuste de la intercepción\n",
    "    \"dual\": [False],  # 'dual=True' solo con 'liblinear', por lo que se mantiene en False\n",
    "    \"class_weight\": [None, \"balanced\"],  # Probar con 'None' y 'balanced'\n",
    "    \"multi_class\": ['auto', 'ovr'],  # Reducir a combinaciones más comunes\n",
    "    'tol': [1e-4, 1e-3],  # Reducir a solo dos valores para tolerancia\n",
    "    'max_iter': [100, 500],  # Reducir el número de iteraciones a 2 opciones\n",
    "    'warm_start': [True, False],  # Continuar desde la última solución\n",
    "    'l1_ratio': np.linspace(0, 1, 3)  # Reducir a 3 valores para l1_ratio\n",
    "}\n",
    "\n",
    "# GridSearchCV con menos combinaciones de parámetros\n",
    "grid = GridSearchCV(\n",
    "    model, param_grid=param_grid, cv=3,  # Usar 3 pliegues para validación cruzada\n",
    "    n_jobs=-1,  # Utilizar todos los núcleos del procesador\n",
    "    verbose=3,  # Mayor nivel de detalle en la salida de la búsqueda\n",
    "    scoring='accuracy',  # Optimizar para precisión\n",
    ")\n",
    "\n",
    "# Ajustamos el grid search al conjunto de datos seleccionado\n",
    "grid.fit(TRAIN_DATASETS[best_dataset_index], y_train)\n",
    "\n",
    "# Obtener el mejor modelo después de GridSearchCV\n",
    "final_model = grid.best_estimator_\n",
    "\n",
    "# Predicciones sobre el conjunto de entrenamiento\n",
    "y_pred_train = final_model.predict(TRAIN_DATASETS[best_dataset_index])\n",
    "\n",
    "# Predicciones sobre el conjunto de prueba\n",
    "y_pred_test = final_model.predict(TEST_DATASETS[best_dataset_index])\n",
    "\n",
    "# Calcular la exactitud\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Guardar los hiperparámetros y las métricas en un archivo JSON\n",
    "data_to_save = {\n",
    "    \"hyperparams\": grid.best_params_,  # Los mejores hiperparámetros encontrados\n",
    "    \"train_accuracy\": train_accuracy,\n",
    "    \"test_accuracy\": test_accuracy\n",
    "}\n",
    "\n",
    "# Guardamos los resultados en un archivo JSON\n",
    "with open(\"../data/hyperparams/hyperparams_2.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f, indent=4)\n",
    "\n",
    "# Imprimir las métricas de rendimiento\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3840 candidates, totalling 11520 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 91.0106\n",
      "Test Accuracy: 90.8958\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np \n",
    "\n",
    "# Asumiendo que TRAIN_DATASETS y TEST_DATASETS son tus conjuntos de datos preprocesados y y_train, y_test son tus etiquetas\n",
    "\n",
    "# Ejemplo de mejor dataset (esto depende de tu código anterior)\n",
    "best_dataset_index = 2  # Esto lo debes definir según el conjunto de datos que hayas seleccionado como el mejor\n",
    "\n",
    "# Define tu modelo y el grid search (si estás haciendo GridSearchCV)\n",
    "model = LogisticRegression(random_state=42, solver='sag', tol=0.1)\n",
    "\n",
    "# Parámetros para GridSearchCV (esto es solo un ejemplo, ajusta según tus necesidades)\n",
    "param_grid = {\n",
    "    \"C\": np.logspace(-3, 3, 5),  # Reducir a 5 valores para C\n",
    "    \"penalty\": [\"l1\", \"l2\"],  # Probar solo l1 y l2\n",
    "    \"solver\": ['liblinear', 'saga'],  # Limitar a 'liblinear' y 'saga', que son más rápidos\n",
    "    \"fit_intercept\": [True, False],  # Mantener el ajuste de la intercepción\n",
    "    \"dual\": [False],  # 'dual=True' solo con 'liblinear', por lo que se mantiene en False\n",
    "    \"class_weight\": [None, \"balanced\"],  # Probar con 'None' y 'balanced'\n",
    "    \"multi_class\": ['auto', 'ovr'],  # Reducir a combinaciones más comunes\n",
    "    'tol': [1e-4, 1e-3],  # Reducir a solo dos valores para tolerancia\n",
    "    'max_iter': [100, 500],  # Reducir el número de iteraciones a 2 opciones\n",
    "    'warm_start': [True, False],  # Continuar desde la última solución\n",
    "    'l1_ratio': np.linspace(0, 1, 3)  # Reducir a 3 valores para l1_ratio\n",
    "}\n",
    "\n",
    "# GridSearchCV con menos combinaciones de parámetros\n",
    "grid = GridSearchCV(\n",
    "    model, param_grid=param_grid, cv=3,  # Usar 3 pliegues para validación cruzada\n",
    "    n_jobs=-1,  # Utilizar todos los núcleos del procesador\n",
    "    verbose=3,  # Mayor nivel de detalle en la salida de la búsqueda\n",
    "    scoring='accuracy',  # Optimizar para precisión\n",
    ")\n",
    "\n",
    "# Ajustamos el grid search al conjunto de datos seleccionado\n",
    "grid.fit(TRAIN_DATASETS[best_dataset_index], y_train)\n",
    "\n",
    "# Obtener el mejor modelo después de GridSearchCV\n",
    "final_model = grid.best_estimator_\n",
    "\n",
    "# Predicciones sobre el conjunto de entrenamiento\n",
    "y_pred_train = final_model.predict(TRAIN_DATASETS[best_dataset_index])\n",
    "\n",
    "# Predicciones sobre el conjunto de prueba\n",
    "y_pred_test = final_model.predict(TEST_DATASETS[best_dataset_index])\n",
    "\n",
    "# Calcular la exactitud\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Guardar los hiperparámetros y las métricas en un archivo JSON\n",
    "data_to_save = {\n",
    "    \"hyperparams\": grid.best_params_,  # Los mejores hiperparámetros encontrados\n",
    "    \"train_accuracy\": train_accuracy,\n",
    "    \"test_accuracy\": test_accuracy\n",
    "}\n",
    "\n",
    "# Guardamos los resultados en un archivo JSON\n",
    "with open(\"../data/hyperparams/hyperparams_2.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f, indent=4)\n",
    "\n",
    "# Imprimir las métricas de rendimiento\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 17280 candidates, totalling 86400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
      "50400 fits failed out of a total of 86400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "4800 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1276, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "                                                ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1228, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py\", line 1060, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "21600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 68, in _check_solver\n",
      "    raise ValueError(f\"Solver {solver} supports only dual=False, got dual={dual}\")\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "14400 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1193, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "9600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1267, in fit\n",
      "    multi_class = _check_multi_class(multi_class, solver, len(self.classes_))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 95, in _check_multi_class\n",
      "    raise ValueError(\"Solver %s does not support a multinomial backend.\" % solver)\n",
      "ValueError: Solver liblinear does not support a multinomial backend.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan ... 0.74397572 0.74412747 0.74412747]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Albert\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 91.0167\n",
      "Test Accuracy: 90.8958\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np \n",
    "\n",
    "# Asumiendo que TRAIN_DATASETS y TEST_DATASETS son tus conjuntos de datos preprocesados y y_train, y_test son tus etiquetas\n",
    "\n",
    "# Ejemplo de mejor dataset (esto depende de tu código anterior)\n",
    "best_dataset_index = 2  # Esto lo debes definir según el conjunto de datos que hayas seleccionado como el mejor\n",
    "\n",
    "# Define tu modelo y el grid search (si estás haciendo GridSearchCV)\n",
    "model = LogisticRegression(random_state=42, solver='sag', tol=0.1)\n",
    "\n",
    "# Parámetros para GridSearchCV (esto es solo un ejemplo, ajusta según tus necesidades)\n",
    "param_grid = {\n",
    "    \"C\": np.logspace(-3, 3, 5),  # Reducir a 3 valores para C\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],  # Limitar a 'l1' y 'l2'\n",
    "    \"solver\": ['liblinear', 'saga'],  # Usar solo 'liblinear'\n",
    "    \"fit_intercept\": [True, False],  # Mantener el ajuste de la intercepción\n",
    "    \"dual\": [True, False],  # 'dual=True' solo con 'liblinear', por lo que se mantiene en False\n",
    "    \"class_weight\": [None, 'balanced'],  # Usar solo 'None'\n",
    "    \"multi_class\": ['auto', 'ovr','multinomial'],  # Usar solo 'auto'\n",
    "    'tol': [1e-2, 1e-3],  # Usar solo 1e-2 para tolerancia\n",
    "    'max_iter': [100, 300, 500],  # Reducir a solo 50 iteraciones\n",
    "    'warm_start': [True, False],  # Evitar continuar desde la última solución\n",
    "    'l1_ratio': [0.5, 0.8]  # Reducir a solo 1 valor para l1_ratio\n",
    "}\n",
    "\n",
    "# GridSearchCV con 2 pliegues para validación cruzada\n",
    "grid = GridSearchCV(\n",
    "    model, param_grid=param_grid, cv=5,  # Usar 2 pliegues para validación cruzada\n",
    "    n_jobs=-1,  # Utilizar todos los núcleos del procesador\n",
    "    verbose=3,  # Mayor nivel de detalle en la salida de la búsqueda\n",
    "    scoring='accuracy',  # Optimizar para precisión\n",
    ")\n",
    "\n",
    "# Ajustamos el grid search al conjunto de datos seleccionado\n",
    "grid.fit(TRAIN_DATASETS[best_dataset_index], y_train)\n",
    "\n",
    "# Obtener el mejor modelo después de GridSearchCV\n",
    "final_model = grid.best_estimator_\n",
    "\n",
    "# Predicciones sobre el conjunto de entrenamiento\n",
    "y_pred_train = final_model.predict(TRAIN_DATASETS[best_dataset_index])\n",
    "\n",
    "# Predicciones sobre el conjunto de prueba\n",
    "y_pred_test = final_model.predict(TEST_DATASETS[best_dataset_index])\n",
    "\n",
    "# Calcular la exactitud\n",
    "train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "# Guardar los hiperparámetros y las métricas en un archivo JSON\n",
    "data_to_save = {\n",
    "    \"hyperparams\": grid.best_params_,  # Los mejores hiperparámetros encontrados\n",
    "    \"train_accuracy\": train_accuracy,\n",
    "    \"test_accuracy\": test_accuracy\n",
    "}\n",
    "\n",
    "# Guardamos los resultados en un archivo JSON\n",
    "with open(\"../data/hyperparams/hyperparams_2.json\", \"w\") as f:\n",
    "    json.dump(data_to_save, f, indent=4)\n",
    "\n",
    "# Imprimir las métricas de rendimiento\n",
    "print(f\"Training Accuracy: {train_accuracy * 100:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigue estando overfitted. Por lo que nos quedaremos con el modelo sin hiperparametros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8942447790189413\n",
      "Precision: 0.6037344398340249\n",
      "Recall: 0.2996910401647786\n",
      "F1 Score: 0.4005505849965588\n",
      "Confusion Matrix:\n",
      " [[7074  191]\n",
      " [ 680  291]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "# Asumiendo que ya tienes y_test y y_pred (y_prob si calculas ROC AUC)\n",
    "acc = accuracy_score(y_test, y_pred_test)\n",
    "prec = precision_score(y_test, y_pred_test)\n",
    "rec = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(\"Precision:\", prec)\n",
    "print(\"Recall:\", rec)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.8981239754720418\n",
    "Precision: 0.6092307692307692\n",
    "Recall: 0.3141195134849286\n",
    "F1 Score: 0.4145150034891835\n",
    "Confusion Matrix:\n",
    " [[14199   381]\n",
    " [ 1297   594]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    df['month'].isin(['jan', 'feb', 'mar']),\n",
    "    df['month'].isin(['apr', 'may', 'jun']),\n",
    "    df['month'].isin(['jul', 'aug', 'sep']),\n",
    "    df['month'].isin(['oct', 'nov', 'dec'])\n",
    "]\n",
    "choices = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "df['qtr'] = np.select(conditions, choices, default=0)\n",
    "df['qtr'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
